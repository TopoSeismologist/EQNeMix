# EQNeMix

Copyright &copy; 2024 R. Ortega, C. Meneses-Ponce & J.D. Castro Morales

## 1. What will you find in this repository?
EQNeMix is a mixed architecture that combines two widely-used neural networks in seismology: **ConvNetQuake** (*Perol et al., 2018*) and **EQTransformer** (*Mousavi et al., 2020*). Our algorithm employs a *Gaussian Mixture Model* to perform a *Bayesian Inference* using the outputs generated by both neural networks. The ultimate outcome is a *probabilistic location* pinpointed using just a *single seismic station*.

* An integral facet of its versatile design is the algorithm's adaptability, as it is not confined to a single travel-time algorithm. It accommodates a spectrum of options ranging from simpler to more intricate travel-time methods. Furthermore, various sampling techniques such as variational inference, Hamiltonian sampling, among others, can be seamlessly integrated. 

* This algorithm is applicable not only to individual seismic stations but can also be extended to entire seismic networks.

## 2. EQTransformer

## 3. ConvNetQuake
2 - Train ConvNetQuake on a dataset

Steps to train the network on a dataset of waveforms:

    Use a catalog of located events and partition them into clusters. This create a new catalog of labeled events. The script is in bin/preprocess.
    Load month long continuous waveform data. Preprocess them (mean removal, normalization). Use the catalog of labeled events to create event windows from the continuous waveform data. Use a catalog to create noise windows from continuous waveform data. The codes are in bin/preprocess.
    Train ConvNetQuake on the training windows, visualize of the training and evaluate on a test windows. The codes are in bin/.

Sections 2.1, 2.2 and 2.3 are required to reproduce the windows in data/6_clusters/detection Section 2.4 for training and testing of the network, also provided in models/convnetquake
2.1 - Partition earthquakes into clusters

Load the OGS catalog. Filter to keep the events located in the region of interest and after 15 February 2014. To partition the events into 6 clusters using K-Means, run:

./bin/preprocess/cluster_events --src data/catalogs/OK_2014-2015-2016.csv\
--dst data/6_clusters --n_components 6 --model KMeans

This outputs in data/6_clusters:

    catalog_with_cluster_ids.csv: catalog of labeled events
    clusters_metadata.json: number of events per clusters.

The code also plots the events on a map. The colored events are the training events, the black events are the events in the test set (July 2014).

The cluster labels range from 0 to M-1 with M the number of clusters chosen with --n-components.
2.2 Create labeled windows of events

Load a directory of month long streams and a catalog of labeled events. The script preprocess the month long streams (remove the mean, normalization). Using the origin time of the cataloged events and a mean seismic velocity between the station and the event location, we create 10 second long event windows.

./bin/preprocess/create_dataset_events.py --stream_dir data/streams\
--catalog data/6_clusters/catalog_with_cluster_ids.csv \
--output_dir data/6_clusters/events \
--save_mseed True --plot True

This create tfrecords containing all the event windows. Pass —-save_mseed to save the windows in .mseed. Pass —-plot to save the events in .png.

data_augmentation.py adds Gaussian noise and can stretch or shift the signal to generate new tfrecords.

./bin/preprocess/data_augmentation.py --tfrecords data/6_clusters/events \
--output data/6_clusters/augmented_data/augmented_stetch_std1-2.tfrecords \
--std_factor 1.2

You can pass various flags: --plot plot the generated windows, -—compress_data compress the signal, -—stretch_data stretch the signal, -—shift_data shifts the signal.

In Perol et al., 2017 we only add Gaussian noise. The other data augmentation techniques do not improve the accuracy of the network.
2.3 Create windows of noise

Load one month long stream and a catalog, preprocess the stream and create noise windows labeled with -1.

./bin/preprocess/create_dataset_noise.py \
--stream_path data/streams/GSOK029_8-2014.mseed \
--catalog data/catalogs/Benz_catalog.csv \
--output_dir data/noise_OK029/noise_august

This generates 10 second long windows when there is no event in the catalog. Check the flags in the code if you want to change this. -—max_windows controls the maximum number of windows to generate. The -—plot and —-save_mseed are available.

Note that in the case we do not account for the travel time because the detection times in Benz et al. 2015 correspond to the detected seismogram signal.
2.4 Train ConvNetQuake and monitor the accuracy on train and test sets

We split the tfrecords of windows for training and testing. The training set has two directories: positive containing the event windows and negative containing the noise windows.

To train ConvNetQuake (GPU recommended):

./bin/train --dataset data/6_clusters/train --checkpoint_dir output/convnetquake --n_clusters 6

This outputs checkpoints with saved weights and tensorboard events in the directory given by the checkpoint_dir flag. The checkpoints are named after the number of steps done during training. For example model-500 correspond to the weights after 500 steps of training. The configuration parameters (batch size, display step etc) are in quakenet/config.py.

The network architecture is stored in quakenet/models.py.

Note that we also provide the trained model in models/convnetquake.

During training, there are two things to monitor: the accuracy on the noise windows and the accuracy on event windows (CPUs are fine here). In both scripts, pass an integer in seconds to --eval_interval to set the time between each evaluation.

./bin/evaluate --checkpoint_dir output/convnetquake/ConvNetQuake \
--dataset data/6_clusters/test_events \
--eval_interval 10 --n_clusters 6 \
--events

./bin/evaluate --checkpoint_dir output/convnetquake/ConvNetQuake \
--dataset data/6_clusters/test_noise --eval_interval 10 \
--n_clusters 6 --noise

You can visualize the accuracy on the train and test set while the network is training. The accuracy for detection and for location is implemented. Run:

tensorboard --logdir output/convnetquake/ConvNetQuake

## 4. EQNeMix
a) 
To merge EQT and CNQ outputs 

  
