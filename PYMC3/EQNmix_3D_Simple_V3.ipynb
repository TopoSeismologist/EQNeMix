{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a106255-d1ee-4d36-977b-051ff8218942",
   "metadata": {},
   "source": [
    "# **EQNmix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e0730-47e3-4ad4-a3c9-636315837d39",
   "metadata": {},
   "source": [
    "### EQNmix is a mixed architecture that combines two widely-used neural networks in seismology: ConvNetQuake (Perol et al., 2018) and EQTransformer (Mousavi et al., 2020). Our algorithm employs a Gaussian mixture model for Bayesian Inference using the outputs generated by both neural networks. The ultimate outcome is a probabilistic location pinpointed using just a single seismic station.ks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58bb84-fbc1-4a28-874e-adcf8343e276",
   "metadata": {},
   "source": [
    "##### An integral facet of its versatile design is the algorithm's adaptability, as it is not confined to a single travel-time algorithm. It accommodates a spectrum of options ranging from simpler to more intricate travel-time methods. Furthermore, various sampling techniques such as variational inference, Hamiltonian sampling, among others, can be seamlessly integrated. \n",
    "##### This algorithm is applicable not only to individual seismic stations but can also be extended to entire seismic networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b35036-095f-4147-adbb-9ad83f6864fa",
   "metadata": {},
   "source": [
    "###### Information of the TEST events obtained by the **Southern California Earthquake Data Center (SCEDC)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789c2be2-cd26-4ea4-b309-1fffca74c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import json\n",
    "from obspy.core import UTCDateTime\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89858578-5903-4ce5-9799-8a48e59cd407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  file_name network station instrument_type  \\\n",
      "2430  CLC_CI_HH_2019-07-05T13:48:48.008300Z      CI    CLC               HH   \n",
      "2454  CLC_CI_HH_2019-07-05T14:01:24.008300Z      CI    CLC               HH   \n",
      "3740  CLC_CI_HH_2019-07-06T01:14:54.008300Z      CI    CLC               HH   \n",
      "6035  CLC_CI_HH_2019-07-06T21:26:36.008300Z      CI    CLC               HH   \n",
      "\n",
      "      station_lat  station_lon  station_elv            event_start_time  \\\n",
      "2430     35.81574   -117.59751          775  2019-07-05 13:49:42.648300   \n",
      "2454     35.81574   -117.59751          775  2019-07-05 14:01:48.688300   \n",
      "3740     35.81574   -117.59751          775  2019-07-06 01:15:22.528300   \n",
      "6035     35.81574   -117.59751          775  2019-07-06 21:27:28.128300   \n",
      "\n",
      "                  event_end_time  detection_probability  \\\n",
      "2430  2019-07-05 13:49:45.238300                   0.99   \n",
      "2454  2019-07-05 14:01:50.498300                   0.99   \n",
      "3740  2019-07-06 01:15:24.568300                   0.98   \n",
      "6035  2019-07-06 21:27:30.888300                   0.99   \n",
      "\n",
      "      detection_uncertainty               p_arrival_time  p_probability  \\\n",
      "2430                    NaN  2019-07-05T13:49:42.618300Z           0.82   \n",
      "2454                    NaN  2019-07-05T14:01:48.678300Z           0.84   \n",
      "3740                    NaN  2019-07-06T01:15:22.528300Z           0.83   \n",
      "6035                    NaN  2019-07-06T21:27:28.108300Z           0.85   \n",
      "\n",
      "      p_uncertainty  p_snr               s_arrival_time  s_probability  \\\n",
      "2430            NaN   10.2  2019-07-05T13:49:43.468300Z           0.89   \n",
      "2454            NaN   31.7  2019-07-05T14:01:49.448300Z           0.89   \n",
      "3740            NaN   17.4  2019-07-06T01:15:23.158300Z           0.89   \n",
      "6035            NaN   25.5  2019-07-06T21:27:29.268300Z           0.90   \n",
      "\n",
      "      s_uncertainty  s_snr t_observed  \n",
      "2430            NaN    6.4       0.85  \n",
      "2454            NaN   11.1       0.77  \n",
      "3740            NaN    5.2       0.63  \n",
      "6035            NaN    5.4       1.16  \n"
     ]
    }
   ],
   "source": [
    "# Read EQT output file\n",
    "eqt_output = '/Users/jorge/EQTransformer/examples/detectionsCLC/CLC_outputs/X_prediction_results.csv'\n",
    "df = pd.read_csv(eqt_output)\n",
    "# Filter events in the dataframe\n",
    "df_filtered = df[(df['detection_probability'] > 0.95) & \n",
    "                 (df['s_probability'] > 0.88) & \n",
    "                 (df['p_probability'] > 0.80)].copy()  # Create a copy\n",
    "# Apply UTCDateTime transformation to the copy\n",
    "df_filtered['p_arrival_time'] = pd.to_datetime(df_filtered['p_arrival_time']).apply(UTCDateTime)\n",
    "df_filtered['s_arrival_time'] = pd.to_datetime(df_filtered['s_arrival_time']).apply(UTCDateTime)\n",
    "# Calculate the difference between S and P arrival times\n",
    "df_filtered['t_observed'] = df_filtered['s_arrival_time'] - df_filtered['p_arrival_time']\n",
    "t_observed = df_filtered['t_observed'].iloc[0]\n",
    "# Show df_filtered\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073c3275-321c-4c02-b404-c52226f4beb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select reference system: STA or TT\n",
    "ref = 'TT'\n",
    "# Choose dimensionality: 2D or 3D\n",
    "dim = '3D'\n",
    "# Upload json files\n",
    "ellipse_data = []\n",
    "datos=[]\n",
    "for i in range(6):\n",
    "    file_path = f'/Users/cecilia/CONVN/data/6_clusters/csv_clusters/{dim}_{ref}/ellipse_parameters_{dim}_{ref}_{i}'\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    ellipse_data.append(data)\n",
    "cov_matrices = []\n",
    "for i in range(6):\n",
    "    cov_matrices.append(np.array(ellipse_data[i]['Covariance']))\n",
    "#clusters_prob = search_times['clusters_prob']\n",
    "# Read CNQ output file\n",
    "means = []\n",
    "for i in range(6):\n",
    "    means.append(np.array(ellipse_data[i]['Mean']))\n",
    "cnq_output = '/Users/cecilia/CONVN/output/july_detections/from_stream/CI.CLC.2019-07-05.csv'\n",
    "df_cnq = pd.read_csv(cnq_output)\n",
    "# Extract P wave arrival times infromation from EQT filtered catalog\n",
    "p_arrival_time = df_filtered['p_arrival_time'].iloc[1]\n",
    "p_times = UTCDateTime(p_arrival_time)\n",
    "# Filter CNQ Dataframe to find where p_times is in between start_time and end_time\n",
    "find_times = df_cnq[(df_cnq['start_time'] <= p_times) & (df_cnq['end_time'] >= p_times)]\n",
    "clusters_prob = find_times['clusters_prob']\n",
    "clusters_weight = clusters_prob.tolist()[0]\n",
    "clusters_weight_i = eval(clusters_weight)\n",
    "w0 = clusters_weight_i[0]\n",
    "w1 = clusters_weight_i[1]\n",
    "w2 = clusters_weight_i[2]\n",
    "w3 = clusters_weight_i[3]\n",
    "w4 = clusters_weight_i[4]\n",
    "w5 = clusters_weight_i[5]\n",
    "weights = [w0, w1, w2, w3, w4, w5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3722df-23d4-46c2-8e8e-ca337abab915",
   "metadata": {},
   "source": [
    "### TEST EVENT A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8a7abe-7c05-443d-b018-1746c1eedccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The t_observed value by EQT is: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniforge3/envs/pymc3EQT/lib/python3.10/site-packages/deprecat/classic.py:215: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  return wrapped_(*args_, **kwargs_)\n",
      "Only 300 samples in chain.\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "CompoundStep\n",
      ">CategoricalGibbsMetropolis: [category]\n",
      ">NUTS: [mu5, mu4, mu3, mu2, mu1, mu0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='1400' class='' max='1400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1400/1400 00:09&lt;00:00 Sampling 4 chains, 1 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 50 tune and 300 draw iterations (200 + 1_200 draws total) took 18 seconds.\n",
      "There was 1 divergence after tuning. Increase `target_accept` or reparameterize.\n",
      "The rhat statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n",
      "Got error No model on context stack. trying to find log_likelihood in translation.\n",
      "/opt/miniforge3/envs/pymc3EQT/lib/python3.10/site-packages/arviz/data/io_pymc3_3x.py:98: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                mean        sd     hdi_3%     hdi_97%  mcse_mean  mcse_sd  \\\n",
      "category       3.358     0.805      3.000       5.000      0.172    0.124   \n",
      "mu0[0]     31933.864  4906.535  22152.189   40506.226    179.888  127.644   \n",
      "mu0[1]    100151.170  5084.953  90605.492  109762.189    162.829  115.172   \n",
      "mu0[2]      4823.747  1871.170   1585.359    8612.759     87.509   65.623   \n",
      "mu1[0]     47355.618  2828.807  42005.783   52574.510    101.314   72.509   \n",
      "mu1[1]     75343.362  5183.481  65833.771   85232.692    193.633  136.975   \n",
      "mu1[2]      7967.098  2858.386   2362.264   13150.532    129.181   93.212   \n",
      "mu2[0]     70358.726  6298.888  57860.266   81844.431    250.617  177.295   \n",
      "mu2[1]     85310.890  5462.806  74658.275   94906.451    154.515  109.973   \n",
      "mu2[2]      5394.625  1641.549   2623.008    8541.727     74.421   52.655   \n",
      "mu3[0]     58450.894  3275.488  52628.712   64295.725    103.757   73.389   \n",
      "mu3[1]     55467.935  3922.629  50098.831   64449.570    181.812  129.920   \n",
      "mu3[2]      8444.218  2293.937   4293.115   12858.226     97.594   70.813   \n",
      "mu4[0]     29699.052  6319.186  18268.277   41874.550    216.813  154.768   \n",
      "mu4[1]     17444.775  3912.392  10071.502   24678.450    138.793  100.349   \n",
      "mu4[2]      8986.492  1726.557   5625.739   12017.853     61.933   46.073   \n",
      "mu5[0]     67600.051  3285.989  61634.552   73898.423    277.697  196.772   \n",
      "mu5[1]     48553.950  4618.010  40404.062   56241.381    401.866  293.695   \n",
      "mu5[2]      7524.883  2395.643   2986.070   12036.249     96.396   68.195   \n",
      "x          59559.546  3918.808  53190.796   66595.731    359.079  259.589   \n",
      "y          55155.670  3725.206  50442.319   63823.636    224.855  161.682   \n",
      "z           8253.553  2347.378   3781.310   12681.855     96.289   68.120   \n",
      "\n",
      "          ess_bulk  ess_tail  r_hat  \n",
      "category      22.0      23.0   1.15  \n",
      "mu0[0]       745.0     694.0   1.00  \n",
      "mu0[1]       987.0     631.0   1.00  \n",
      "mu0[2]       452.0     476.0   1.01  \n",
      "mu1[0]       774.0     427.0   1.01  \n",
      "mu1[1]       724.0     354.0   1.00  \n",
      "mu1[2]       489.0     581.0   1.01  \n",
      "mu2[0]       633.0     422.0   1.01  \n",
      "mu2[1]      1238.0     680.0   1.01  \n",
      "mu2[2]       519.0     505.0   1.01  \n",
      "mu3[0]       984.0     822.0   1.00  \n",
      "mu3[1]       516.0     444.0   1.00  \n",
      "mu3[2]       552.0     414.0   1.01  \n",
      "mu4[0]       844.0     449.0   1.00  \n",
      "mu4[1]       799.0     540.0   1.00  \n",
      "mu4[2]       801.0     540.0   1.00  \n",
      "mu5[0]       145.0     346.0   1.03  \n",
      "mu5[1]       131.0     348.0   1.03  \n",
      "mu5[2]       604.0     520.0   1.02  \n",
      "x            140.0     599.0   1.04  \n",
      "y            401.0     271.0   1.01  \n",
      "z            593.0     399.0   1.01  \n"
     ]
    }
   ],
   "source": [
    "print(f\"The t_observed value by EQT is: {t_observed}\")\n",
    "# Define specific bidimensional means for each ellipse calculated in \n",
    "# Building_Confidence_Ellipses_meters.ipynb (category) [METERS]\n",
    "# Define the function S_P_t (Theoretical traveltime function) [SECONDS]\n",
    "def S_P_t(x, y,z):\n",
    "    filename=\"/Users/roberto/LIATRAB/EQNeMix/PYEIFMM/tsp.npy\"\n",
    "    tsp = np.load(filename)\n",
    "    tsp2 = theano.shared(tsp) \n",
    "    X_rounded = tt.cast(tt.floor_div(x, 500) * 500, 'int64')\n",
    "    Y_rounded = tt.cast(tt.floor_div(y, 500) * 500, 'int64')\n",
    "    Z_rounded = tt.cast(tt.floor_div(z, 500) * 500, 'int64')\n",
    "    # Find the corresponding indices in the tsp array\n",
    "    x_index = X_rounded // 500\n",
    "    y_index = Y_rounded // 500\n",
    "    z_index = Z_rounded // 500\n",
    "    tval=tsp2[x_index,y_index,z_index]\n",
    "    return tval\n",
    "\n",
    "# Define the Bayesian model\n",
    "with pm.Model() as model:\n",
    "    # Define the categories to choose the means\n",
    "    category = pm.Categorical('category', p=weights)\n",
    "\n",
    "    # Define the means corresponding to the categories\n",
    "    mus = [pm.MvNormal(f'mu{i}', mu=means[i], cov=cov_matrices[i], shape=3) for i in range(len(weights))]\n",
    "\n",
    "    # Select the averages corresponding to the selected category.\n",
    "    x = pm.Deterministic('x', pm.math.switch(\n",
    "        pm.math.eq(category, 0), mus[0][0],\n",
    "        pm.math.switch(pm.math.eq(category, 1), mus[1][0],\n",
    "        pm.math.switch(pm.math.eq(category, 2), mus[2][0],\n",
    "        pm.math.switch(pm.math.eq(category, 3), mus[3][0],\n",
    "        pm.math.switch(pm.math.eq(category, 4), mus[4][0], mus[5][0]))))))\n",
    "    \n",
    "    y = pm.Deterministic('y', pm.math.switch(\n",
    "        pm.math.eq(category, 0), mus[0][1],\n",
    "        pm.math.switch(pm.math.eq(category, 1), mus[1][1],\n",
    "        pm.math.switch(pm.math.eq(category, 2), mus[2][1],\n",
    "        pm.math.switch(pm.math.eq(category, 3), mus[3][1],\n",
    "        pm.math.switch(pm.math.eq(category, 4), mus[4][1], mus[5][1]))))))\n",
    "\n",
    "    z = pm.Deterministic('z', pm.math.switch(\n",
    "        pm.math.eq(category, 0), mus[0][1],\n",
    "        pm.math.switch(pm.math.eq(category, 1), mus[1][2],\n",
    "        pm.math.switch(pm.math.eq(category, 2), mus[2][2],\n",
    "        pm.math.switch(pm.math.eq(category, 3), mus[3][2],\n",
    "        pm.math.switch(pm.math.eq(category, 4), mus[4][2], mus[5][2]))))))\n",
    "    \n",
    "    # Calculate t using the theoretical function\n",
    "    t = S_P_t(x, y, z)\n",
    "\n",
    "    # Likelihood of the observed data\n",
    "    obs = pm.Normal('obs', mu=t, sigma=0.1, observed=t_observed)\n",
    "\n",
    "with model:\n",
    "    trace = pm.sample(300, tune=50, cores=4)\n",
    "\n",
    "# Trace summary\n",
    "summary_df = pm.summary(trace)\n",
    "\n",
    "# Convert summary to dataframe\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "\n",
    "# Show DataFrame\n",
    "print(summary_df)\n",
    "\n",
    "#pm.traceplot(trace)\n",
    "#pm.autocorrplot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b504a7f-8467-4afa-9933-44873f755f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude: -117.54913547167894°\n",
      "Longitude: 35.702019381210256°\n",
      "Depth: 8.253553 [km]\n"
     ]
    }
   ],
   "source": [
    "# Extract 'x', 'y', 'z' from 'mean' column\n",
    "x_utm = summary_df.at['x', 'mean']\n",
    "y_utm = summary_df.at['y', 'mean']\n",
    "z_utm = summary_df.at['z', 'mean']\n",
    "\n",
    "# Transformer from (EPSG:32611 - UTM11N, WGS84) to (EPSG:4326 - lat, long, WGS84)\n",
    "latlon_proj = pyproj.Transformer.from_crs(32611, 4326, always_xy=True)\n",
    "\n",
    "# Transformer from (EPSG:4326 - lat, long, WGS84) to (EPSG:32611 - UTM11N, WGS84)\n",
    "utm_proj = pyproj.Transformer.from_crs(4326, 32611, always_xy=True)\n",
    "\n",
    "# Relative reference origin coordinates\n",
    "ref_latitude = 35.2\n",
    "ref_longitude = -118.2\n",
    "ref_depth = 0\n",
    "\n",
    "# Transform reference coordinates to UTM\n",
    "ref_longitude_utm, ref_latitude_utm = utm_proj.transform(ref_longitude, ref_latitude)\n",
    "\n",
    "# Correct relative reference origin effect\n",
    "x_utm += ref_longitude_utm\n",
    "y_utm += ref_latitude_utm\n",
    "\n",
    "# Convert depth from meters [m] to kilometers [km]\n",
    "z_depth = z_utm/1000\n",
    "\n",
    "# Transform coordinates from UTM to latitude, longitude\n",
    "x_longitude, y_latitude = latlon_proj.transform(x_utm,y_utm)\n",
    "\n",
    "# Print transformed coordinates\n",
    "print(f'Longitude: {x_longitude}°')\n",
    "print(f'Latitude: {y_latitude}°')\n",
    "print(f'Depth: {z_depth} [km]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25e139b-ca99-4785-a648-0346dd54e10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude: -117.54913547167894°\n",
      "Longitude: 35.702019381210256°\n",
      "Depth: 8.253553 [km]\n"
     ]
    }
   ],
   "source": [
    "def metolatlon(x_utm, y_utm, z_utm):\n",
    "    # Transformer from (EPSG:32611 - UTM11N, WGS84) to (EPSG:4326 - lat, long, WGS84)\n",
    "    latlon_proj = pyproj.Transformer.from_crs(32611, 4326, always_xy=True)\n",
    "\n",
    "    # Transformer from (EPSG:4326 - lat, long, WGS84) to (EPSG:32611 - UTM11N, WGS84)\n",
    "    utm_proj = pyproj.Transformer.from_crs(4326, 32611, always_xy=True)\n",
    "\n",
    "    # Relative reference origin coordinates\n",
    "    ref_latitude = 35.2\n",
    "    ref_longitude = -118.2\n",
    "    ref_depth = 0\n",
    "\n",
    "    # Transform reference coordinates to UTM\n",
    "    ref_longitude_utm, ref_latitude_utm = utm_proj.transform(ref_longitude, ref_latitude)\n",
    "\n",
    "    # Correct relative reference origin effect\n",
    "    x_utm += ref_longitude_utm\n",
    "    y_utm += ref_latitude_utm\n",
    "\n",
    "    # Convert depth from meters [m] to kilometers [km]\n",
    "    z_depth = z_utm / 1000\n",
    "\n",
    "    # Transform coordinates from UTM to latitude, longitude\n",
    "    x_longitude, y_latitude = latlon_proj.transform(x_utm, y_utm)\n",
    "\n",
    "    return x_longitude, y_latitude, z_depth\n",
    "\n",
    "# Example...\n",
    "x_utm_example = summary_df.at['x', 'mean']\n",
    "y_utm_example = summary_df.at['y', 'mean']\n",
    "z_utm_example = summary_df.at['z', 'mean']\n",
    "\n",
    "result = metolatlon(x_utm_example, y_utm_example, z_utm_example)\n",
    "\n",
    "print(f'Latitude: {result[0]}°')\n",
    "print(f'Longitude: {result[1]}°')\n",
    "print(f'Depth: {result[2]} [km]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6a7dd-6348-47f3-94b3-ddf5531ce7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a281c6a-ea61-4a22-b3b1-c837c0400e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_white_catalog = 'Users/cecilia/Figuras Tesis Lia/csv/Ridgecrest_filtrado_UTC.csv'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
