{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a106255-d1ee-4d36-977b-051ff8218942",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# **EQNeMix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e0730-47e3-4ad4-a3c9-636315837d39",
   "metadata": {},
   "source": [
    "### EQNmix is a mixed architecture that combines two widely-used neural networks in seismology: ConvNetQuake (Perol et al., 2018) and EQTransformer (Mousavi et al., 2020). Our algorithm employs a Gaussian mixture model for Bayesian Inference using the outputs generated by both neural networks. The ultimate outcome is a probabilistic location pinpointed using just a single seismic station.ks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58bb84-fbc1-4a28-874e-adcf8343e276",
   "metadata": {},
   "source": [
    "##### An integral facet of its versatile design is the algorithm's adaptability, as it is not confined to a single travel-time algorithm. It accommodates a spectrum of options ranging from simpler to more intricate travel-time methods. Furthermore, various sampling techniques such as variational inference, Hamiltonian sampling, among others, can be seamlessly integrated. \n",
    "##### This algorithm is applicable not only to individual seismic stations but can also be extended to entire seismic networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789c2be2-cd26-4ea4-b309-1fffca74c37d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "import json\n",
    "from obspy.core import UTCDateTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e6ac4-f0f9-4af2-bf9a-4d39bad18c96",
   "metadata": {},
   "source": [
    "#### **STEP 1:** Obtain events from EQTransformer prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfa9f7bb-d1dd-4f2f-bed7-0660de01fd16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>network</th>\n",
       "      <th>station</th>\n",
       "      <th>instrument_type</th>\n",
       "      <th>station_lat</th>\n",
       "      <th>station_lon</th>\n",
       "      <th>station_elv</th>\n",
       "      <th>event_start_time</th>\n",
       "      <th>event_end_time</th>\n",
       "      <th>detection_probability</th>\n",
       "      <th>detection_uncertainty</th>\n",
       "      <th>p_arrival_time</th>\n",
       "      <th>p_probability</th>\n",
       "      <th>p_uncertainty</th>\n",
       "      <th>p_snr</th>\n",
       "      <th>s_arrival_time</th>\n",
       "      <th>s_probability</th>\n",
       "      <th>s_uncertainty</th>\n",
       "      <th>s_snr</th>\n",
       "      <th>t_observed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>CLC_CI_HH_2019-07-05T13:48:48.008300Z</td>\n",
       "      <td>CI</td>\n",
       "      <td>CLC</td>\n",
       "      <td>HH</td>\n",
       "      <td>35.81574</td>\n",
       "      <td>-117.59751</td>\n",
       "      <td>775</td>\n",
       "      <td>2019-07-05 13:49:42.648300</td>\n",
       "      <td>2019-07-05 13:49:45.238300</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-05T13:49:42.618300Z</td>\n",
       "      <td>0.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.2</td>\n",
       "      <td>2019-07-05T13:49:43.468300Z</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.4</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>CLC_CI_HH_2019-07-05T14:01:24.008300Z</td>\n",
       "      <td>CI</td>\n",
       "      <td>CLC</td>\n",
       "      <td>HH</td>\n",
       "      <td>35.81574</td>\n",
       "      <td>-117.59751</td>\n",
       "      <td>775</td>\n",
       "      <td>2019-07-05 14:01:48.688300</td>\n",
       "      <td>2019-07-05 14:01:50.498300</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-05T14:01:48.678300Z</td>\n",
       "      <td>0.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.7</td>\n",
       "      <td>2019-07-05T14:01:49.448300Z</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>CLC_CI_HH_2019-07-06T01:14:54.008300Z</td>\n",
       "      <td>CI</td>\n",
       "      <td>CLC</td>\n",
       "      <td>HH</td>\n",
       "      <td>35.81574</td>\n",
       "      <td>-117.59751</td>\n",
       "      <td>775</td>\n",
       "      <td>2019-07-06 01:15:22.528300</td>\n",
       "      <td>2019-07-06 01:15:24.568300</td>\n",
       "      <td>0.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-06T01:15:22.528300Z</td>\n",
       "      <td>0.83</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.4</td>\n",
       "      <td>2019-07-06T01:15:23.158300Z</td>\n",
       "      <td>0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.2</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>CLC_CI_HH_2019-07-06T21:26:36.008300Z</td>\n",
       "      <td>CI</td>\n",
       "      <td>CLC</td>\n",
       "      <td>HH</td>\n",
       "      <td>35.81574</td>\n",
       "      <td>-117.59751</td>\n",
       "      <td>775</td>\n",
       "      <td>2019-07-06 21:27:28.128300</td>\n",
       "      <td>2019-07-06 21:27:30.888300</td>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-06T21:27:28.108300Z</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.5</td>\n",
       "      <td>2019-07-06T21:27:29.268300Z</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  file_name network station instrument_type  \\\n",
       "2430  CLC_CI_HH_2019-07-05T13:48:48.008300Z      CI    CLC               HH   \n",
       "2454  CLC_CI_HH_2019-07-05T14:01:24.008300Z      CI    CLC               HH   \n",
       "3740  CLC_CI_HH_2019-07-06T01:14:54.008300Z      CI    CLC               HH   \n",
       "6035  CLC_CI_HH_2019-07-06T21:26:36.008300Z      CI    CLC               HH   \n",
       "\n",
       "      station_lat  station_lon  station_elv            event_start_time  \\\n",
       "2430     35.81574   -117.59751          775  2019-07-05 13:49:42.648300   \n",
       "2454     35.81574   -117.59751          775  2019-07-05 14:01:48.688300   \n",
       "3740     35.81574   -117.59751          775  2019-07-06 01:15:22.528300   \n",
       "6035     35.81574   -117.59751          775  2019-07-06 21:27:28.128300   \n",
       "\n",
       "                  event_end_time  detection_probability  \\\n",
       "2430  2019-07-05 13:49:45.238300                   0.99   \n",
       "2454  2019-07-05 14:01:50.498300                   0.99   \n",
       "3740  2019-07-06 01:15:24.568300                   0.98   \n",
       "6035  2019-07-06 21:27:30.888300                   0.99   \n",
       "\n",
       "      detection_uncertainty               p_arrival_time  p_probability  \\\n",
       "2430                    NaN  2019-07-05T13:49:42.618300Z           0.82   \n",
       "2454                    NaN  2019-07-05T14:01:48.678300Z           0.84   \n",
       "3740                    NaN  2019-07-06T01:15:22.528300Z           0.83   \n",
       "6035                    NaN  2019-07-06T21:27:28.108300Z           0.85   \n",
       "\n",
       "      p_uncertainty  p_snr               s_arrival_time  s_probability  \\\n",
       "2430            NaN   10.2  2019-07-05T13:49:43.468300Z           0.89   \n",
       "2454            NaN   31.7  2019-07-05T14:01:49.448300Z           0.89   \n",
       "3740            NaN   17.4  2019-07-06T01:15:23.158300Z           0.89   \n",
       "6035            NaN   25.5  2019-07-06T21:27:29.268300Z           0.90   \n",
       "\n",
       "      s_uncertainty  s_snr t_observed  \n",
       "2430            NaN    6.4       0.85  \n",
       "2454            NaN   11.1       0.77  \n",
       "3740            NaN    5.2       0.63  \n",
       "6035            NaN    5.4       1.16  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read EQT output file\n",
    "eqt_output = '/Users/jorge/EQTransformer/examples/detectionsCLC/CLC_outputs/X_prediction_results.csv'\n",
    "df = pd.read_csv(eqt_output)\n",
    "\n",
    "# Filter events in the dataframe\n",
    "df_filtered = df[(df['detection_probability'] > 0.95) & \n",
    "                 (df['s_probability'] > 0.88) & \n",
    "                 (df['p_probability'] > 0.80)].copy()\n",
    "\n",
    "# Apply UTCDateTime transformation\n",
    "df_filtered['p_arrival_time'] = pd.to_datetime(df_filtered['p_arrival_time']).apply(UTCDateTime)\n",
    "df_filtered['s_arrival_time'] = pd.to_datetime(df_filtered['s_arrival_time']).apply(UTCDateTime)\n",
    "\n",
    "# Calculate the difference between S and P arrival times\n",
    "df_filtered['t_observed'] = df_filtered['s_arrival_time'] - df_filtered['p_arrival_time']\n",
    "\n",
    "t_observed = df_filtered['t_observed'].iloc[0]\n",
    "\n",
    "# Show dataframe filtered\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d455c3b-4791-4109-b36e-94df206d1baa",
   "metadata": {},
   "source": [
    "#### **STEP 2:** Upload covariance ellipses information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8915b-b164-4dbf-af49-631437bf3ae9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select reference system: STA or TT\n",
    "ref = 'TT'\n",
    "\n",
    "# Choose dimensionality: 2D or 3D\n",
    "dim = '3D'\n",
    "\n",
    "# Upload json files\n",
    "ellipse_data = []\n",
    "for i in range(6):\n",
    "    file_path = f'/Users/cecilia/CONVN/data/6_clusters/csv_clusters/{dim}_{ref}/ellipse_parameters_{dim}_{ref}_{i}.json'\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    datos.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b8869c-8e0f-4b31-a92b-86ec7fc9bffa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read covariance matrices\n",
    "cov_matrices = []\n",
    "for i in range(6):\n",
    "    cov_matrices.append(np.array(ellipse_data[i]['Covariance']))\n",
    "\n",
    "# Show covariance matrices\n",
    "cov_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00ff5d-0c65-43e2-9434-72b2d74cc53e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read means\n",
    "mus = []\n",
    "for i in range(6):\n",
    "    mus.append(np.array(ellipse_data[i]['Mean']))\n",
    "\n",
    "# Show means\n",
    "mus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db71c2-d73e-40f7-a2e3-8c9a4703b874",
   "metadata": {},
   "source": [
    "#### **STEP 3:** Search events in ConvNetQuake results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd354b5-49ac-4b4a-abc9-ab358047a6ce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read CNQ output file\n",
    "cnq_output = '/Users/cecilia/CONVN/output/july_detections/from_stream/CI.CLC.2019-07-05.csv'\n",
    "df_cnq = pd.read_csv(cnq_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f323f2-8880-4b76-9f2c-65e458f27b88",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract P wave arrival times infromation from EQT filtered catalog\n",
    "p_arrival_time = df_filtered['p_arrival_time'].iloc[1]\n",
    "p_times = UTCDateTime(p_arrival_time)\n",
    "\n",
    "# Filter CNQ Dataframe to find where p_times is in between start_time and end_time\n",
    "find_times = df_cnq[(df_cnq['start_time'] <= p_times) & (df_cnq['end_time'] >= p_times)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546c181-6310-4674-9ed2-2d17aafe3a21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusters_prob = search_times['clusters_prob']\n",
    "clusters_weight = clusters_prob.tolist()[0]\n",
    "clusters_weight_i = eval(clusters_weight)\n",
    "\n",
    "w0 = clusters_weight_i[0]\n",
    "w1 = clusters_weight_i[1]\n",
    "w2 = clusters_weight_i[2]\n",
    "w3 = clusters_weight_i[3]\n",
    "w4 = clusters_weight_i[4]\n",
    "w5 = clusters_weight_i[5]\n",
    "\n",
    "weights = [w0, w1, w2, w3, w4, w5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3722df-23d4-46c2-8e8e-ca337abab915",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### **STEP 4:** Execute Bayesian Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8a7abe-7c05-443d-b018-1746c1eedccf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Observed value of S-P is given by EQTransformer [SECONDS]\n",
    "ts_observed = 35.618300\n",
    "tp_observed = 34.068300\n",
    "t_observed = ts_observed - tp_observed\n",
    "print(f\"The t_observed value by EQT is: {t_observed}\")\n",
    "\n",
    "# Define the function S_P_t (Theoretical traveltime function) [SECONDS]\n",
    "def S_P_t(x, y):\n",
    "    st_loc = [1, 3]\n",
    "    p_velocity = 7100   #[METERS/SECOND]\n",
    "    s_velocity = 2900   #[METERS/SECOND]\n",
    "    lent = (1 / s_velocity - 1 / p_velocity)\n",
    "    dis = np.sqrt((x - st_loc[0]) ** 2 + (y - st_loc[1]) ** 2)\n",
    "    sminp = dis * lent\n",
    "    return sminp\n",
    "\n",
    "# Define the Bayesian model\n",
    "with pm.Model() as model:\n",
    "    # Define the categories to choose the means\n",
    "    category = pm.Categorical('category', p=weights)\n",
    "\n",
    "    # Define the means corresponding to the categories\n",
    "    mus = [pm.MvNormal(f'mu{i}', mu=mus[i], cov=cov_matrices[i], shape=2) for i in range(len(weights))]\n",
    "\n",
    "    # Select the averages corresponding to the selected category.\n",
    "    x = pm.Deterministic('x', pm.math.switch(\n",
    "        pm.math.eq(category, 0), mus[0][0],\n",
    "        pm.math.switch(pm.math.eq(category, 1), mus[1][0],\n",
    "        pm.math.switch(pm.math.eq(category, 2), mus[2][0],\n",
    "        pm.math.switch(pm.math.eq(category, 3), mus[3][0],\n",
    "        pm.math.switch(pm.math.eq(category, 4), mus[4][0], mus[5][0]))))))\n",
    "    \n",
    "    y = pm.Deterministic('y', pm.math.switch(\n",
    "        pm.math.eq(category, 0), mus[0][1],\n",
    "        pm.math.switch(pm.math.eq(category, 1), mus[1][1],\n",
    "        pm.math.switch(pm.math.eq(category, 2), mus[2][1],\n",
    "        pm.math.switch(pm.math.eq(category, 3), mus[3][1],\n",
    "        pm.math.switch(pm.math.eq(category, 4), mus[4][1], mus[5][1]))))))\n",
    "        \n",
    "    # Calculate t using the theoretical function\n",
    "    t = S_P_t(x, y)\n",
    "\n",
    "    # Likelihood of the observed data\n",
    "    obs = pm.Normal('obs', mu=t, sigma=0.1, observed=t_observed)\n",
    "\n",
    "with model:\n",
    "    trace = pm.sample(300, tune=50, cores=1)\n",
    "\n",
    "# Results summary\n",
    "pm.summary(trace)\n",
    "\n",
    "#pm.traceplot(trace)\n",
    "#pm.autocorrplot(trace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
