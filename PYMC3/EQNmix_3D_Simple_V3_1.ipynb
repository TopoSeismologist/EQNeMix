{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a106255-d1ee-4d36-977b-051ff8218942",
   "metadata": {},
   "source": [
    "# **EQNmix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e0730-47e3-4ad4-a3c9-636315837d39",
   "metadata": {},
   "source": [
    "### EQNmix is a mixed architecture that combines two widely-used neural networks in seismology: ConvNetQuake (Perol et al., 2018) and EQTransformer (Mousavi et al., 2020). Our algorithm employs a Gaussian mixture model for Bayesian Inference using the outputs generated by both neural networks. The ultimate outcome is a probabilistic location pinpointed using just a single seismic station.ks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58bb84-fbc1-4a28-874e-adcf8343e276",
   "metadata": {},
   "source": [
    "##### An integral facet of its versatile design is the algorithm's adaptability, as it is not confined to a single travel-time algorithm. It accommodates a spectrum of options ranging from simpler to more intricate travel-time methods. Furthermore, various sampling techniques such as variational inference, Hamiltonian sampling, among others, can be seamlessly integrated. \n",
    "##### This algorithm is applicable not only to individual seismic stations but can also be extended to entire seismic networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b35036-095f-4147-adbb-9ad83f6864fa",
   "metadata": {},
   "source": [
    "###### Information of the TEST events obtained by the **Southern California Earthquake Data Center (SCEDC)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893d1989-ee0f-4008-ad91-2883205fcfd0",
   "metadata": {},
   "source": [
    "##### **Event A** \\| 2019/07/04 19\\:21:32.09 eq  l 4.50 w   35.67150 -117.47883   5.2 A 38443871  120 3331\n",
    "###### Mixing coefficients by CNQ: \\[0.06642566, 0.13303314, 0.018152032, 0.2676338, 0.03821565, 0.27928686\\]\n",
    "           \n",
    "##### **Event B** \\| 2019/07/05 12\\:38:30.02 eq  l 4.09 w   35.77167 -117.57067   6.8 A 38451079  107 3341\n",
    "###### Mixing coefficients by CNQ: \\[0.06656365, 0.13407934, 0.018142378, 0.2691841, 0.038651247, 0.2805622\\]\n",
    "\n",
    "##### **Event C** \\| 2019/07/06 23\\:50:41.99 eq  l 4.50 w   35.82350 -117.66300   6.5 A 38469375  210 2460\n",
    "###### Mixing coefficients by CNQ: \\[0.0666608, 0.13491394, 0.018160287, 0.27093622, 0.03867901, 0.28238913\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789c2be2-cd26-4ea4-b309-1fffca74c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import json\n",
    "from obspy.core import UTCDateTime\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89858578-5903-4ce5-9799-8a48e59cd407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read EQT output file\n",
    "eqt_output = 'X_prediction_results.csv'\n",
    "df = pd.read_csv(eqt_output)\n",
    "# Filter events in the dataframe\n",
    "df_filtered = df[(df['detection_probability'] > 0.95) & \n",
    "                 (df['s_probability'] > 0.88) & \n",
    "                 (df['p_probability'] > 0.80)].copy()  # Create a copy\n",
    "# Apply UTCDateTime transformation to the copy\n",
    "df_filtered['p_arrival_time'] = pd.to_datetime(df_filtered['p_arrival_time']).apply(UTCDateTime)\n",
    "df_filtered['s_arrival_time'] = pd.to_datetime(df_filtered['s_arrival_time']).apply(UTCDateTime)\n",
    "# Calculate the difference between S and P arrival times\n",
    "df_filtered['t_observed'] = df_filtered['s_arrival_time'] - df_filtered['p_arrival_time']\n",
    "t_observed = df_filtered['t_observed'].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "073c3275-321c-4c02-b404-c52226f4beb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m find_times \u001b[38;5;241m=\u001b[39m df_cnq[(df_cnq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m p_times) \u001b[38;5;241m&\u001b[39m (df_cnq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m p_times)]\n\u001b[1;32m     28\u001b[0m clusters_prob \u001b[38;5;241m=\u001b[39m find_times[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclusters_prob\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 29\u001b[0m clusters_weight \u001b[38;5;241m=\u001b[39m \u001b[43mclusters_prob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     30\u001b[0m clusters_weight_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(clusters_weight)\n\u001b[1;32m     31\u001b[0m w0 \u001b[38;5;241m=\u001b[39m clusters_weight_i[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Select reference system: STA or TT\n",
    "ref = 'TT'\n",
    "# Choose dimensionality: 2D or 3D\n",
    "dim = '3D'\n",
    "# Upload json files\n",
    "ellipse_data = []\n",
    "datos=[]\n",
    "for i in range(6):\n",
    "    file_path = f'/Users/cecilia/CONVN/data/6_clusters/csv_clusters/{dim}_{ref}/ellipse_parameters_{dim}_{ref}_{i}'\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    ellipse_data.append(data)\n",
    "cov_matrices = []\n",
    "for i in range(6):\n",
    "    cov_matrices.append(np.array(ellipse_data[i]['Covariance']))\n",
    "#clusters_prob = search_times['clusters_prob']\n",
    "# Read CNQ output file\n",
    "mus = []\n",
    "for i in range(6):\n",
    "    mus.append(np.array(ellipse_data[i]['Mean']))\n",
    "cnq_output = '/Users/cecilia/CONVN/output/july_detections/from_stream/CI.CLC.2019-07-05.csv'\n",
    "df_cnq = pd.read_csv(cnq_output)\n",
    "# Extract P wave arrival times infromation from EQT filtered catalog\n",
    "p_arrival_time = df_filtered['p_arrival_time'].iloc[3]\n",
    "p_times = UTCDateTime(p_arrival_time)\n",
    "# Filter CNQ Dataframe to find where p_times is in between start_time and end_time\n",
    "find_times = df_cnq[(df_cnq['start_time'] <= p_times) & (df_cnq['end_time'] >= p_times)]\n",
    "clusters_prob = find_times['clusters_prob']\n",
    "clusters_weight = clusters_prob.tolist()[2]\n",
    "clusters_weight_i = eval(clusters_weight)\n",
    "w0 = clusters_weight_i[0]\n",
    "w1 = clusters_weight_i[1]\n",
    "w2 = clusters_weight_i[2]\n",
    "w3 = clusters_weight_i[3]\n",
    "w4 = clusters_weight_i[4]\n",
    "w5 = clusters_weight_i[5]\n",
    "weights = [w0, w1, w2, w3, w4, w5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3722df-23d4-46c2-8e8e-ca337abab915",
   "metadata": {},
   "source": [
    "### TEST EVENT A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8a7abe-7c05-443d-b018-1746c1eedccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The t_observed value by EQT is: 1.16\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Define the Bayesian model\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pm\u001b[38;5;241m.\u001b[39mModel() \u001b[38;5;28;01mas\u001b[39;00m model:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Define the categories to choose the means\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     category \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39mCategorical(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m, p\u001b[38;5;241m=\u001b[39m\u001b[43mweights\u001b[49m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# Define the means corresponding to the categories\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     mus \u001b[38;5;241m=\u001b[39m [pm\u001b[38;5;241m.\u001b[39mMvNormal(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, mu\u001b[38;5;241m=\u001b[39mmus[i], cov\u001b[38;5;241m=\u001b[39mcov_matrices[i], shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(weights))]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weights' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"The t_observed value by EQT is: {t_observed}\")\n",
    "# Define specific bidimensional means for each ellipse calculated in \n",
    "# Building_Confidence_Ellipses_meters.ipynb (category) [METERS]\n",
    "# Define the function S_P_t (Theoretical traveltime function) [SECONDS]\n",
    "def S_P_t(x, y,z):\n",
    "    filename=\"/Users/roberto/LIATRAB/EQNeMix/PYEIFMM/tsp.npy\"\n",
    "    tsp = np.load(filename)\n",
    "    tsp2 = theano.shared(tsp) \n",
    "    X_rounded = tt.cast(tt.floor_div(x, 500) * 500, 'int64')\n",
    "    Y_rounded = tt.cast(tt.floor_div(y, 500) * 500, 'int64')\n",
    "    Z_rounded = tt.cast(tt.floor_div(z, 500) * 500, 'int64')\n",
    "    # Find the corresponding indices in the tsp array\n",
    "    x_index = X_rounded // 500\n",
    "    y_index = Y_rounded // 500\n",
    "    z_index = Z_rounded // 500\n",
    "    tval=tsp2[x_index,y_index,z_index]\n",
    "    return tval\n",
    "\n",
    "# Define the Bayesian model\n",
    "with pm.Model() as model:\n",
    "    # Define the categories to choose the means\n",
    "    category = pm.Categorical('category', p=weights)\n",
    "\n",
    "    # Define the means corresponding to the categories\n",
    "    mus = [pm.MvNormal(f'mu{i}', mu=mus[i], cov=cov_matrices[i], shape=3) for i in range(len(weights))]\n",
    "\n",
    "    # Select the averages corresponding to the selected category.\n",
    "    x = pm.Deterministic('x', pm.math.switch(\n",
    "        pm.math.eq(category, 0), mus[0][0],\n",
    "        pm.math.switch(pm.math.eq(category, 1), mus[1][0],\n",
    "        pm.math.switch(pm.math.eq(category, 2), mus[2][0],\n",
    "        pm.math.switch(pm.math.eq(category, 3), mus[3][0],\n",
    "        pm.math.switch(pm.math.eq(category, 4), mus[4][0], mus[5][0]))))))\n",
    "    \n",
    "    y = pm.Deterministic('y', pm.math.switch(\n",
    "        pm.math.eq(category, 0), mus[0][1],\n",
    "        pm.math.switch(pm.math.eq(category, 1), mus[1][1],\n",
    "        pm.math.switch(pm.math.eq(category, 2), mus[2][1],\n",
    "        pm.math.switch(pm.math.eq(category, 3), mus[3][1],\n",
    "        pm.math.switch(pm.math.eq(category, 4), mus[4][1], mus[5][1]))))))\n",
    "\n",
    "    z = pm.Deterministic('z', pm.math.switch(\n",
    "        pm.math.eq(category, 0), mus[0][1],\n",
    "        pm.math.switch(pm.math.eq(category, 1), mus[1][2],\n",
    "        pm.math.switch(pm.math.eq(category, 2), mus[2][2],\n",
    "        pm.math.switch(pm.math.eq(category, 3), mus[3][2],\n",
    "        pm.math.switch(pm.math.eq(category, 4), mus[4][2], mus[5][2]))))))\n",
    "    \n",
    "    # Calculate t using the theoretical function\n",
    "    t = S_P_t(x, y, z)\n",
    "\n",
    "    # Likelihood of the observed data\n",
    "    obs = pm.Normal('obs', mu=t, sigma=0.1, observed=t_observed)\n",
    "\n",
    "with model:\n",
    "    trace = pm.sample(300, tune=50, cores = 1)\n",
    "\n",
    "# Trace summary\n",
    "summary_df = pm.summary(trace)\n",
    "\n",
    "# Convert summary to dataframe\n",
    "summary_df = pd.DataFrame(summary_df)\n",
    "\n",
    "# Show DataFrame\n",
    "print(summary_df)\n",
    "\n",
    "#pm.traceplot(trace)\n",
    "#pm.autocorrplot(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b504a7f-8467-4afa-9933-44873f755f53",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Extract 'x', 'y', 'z' from 'mean' column\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m x_utm \u001b[38;5;241m=\u001b[39m \u001b[43msummary_df\u001b[49m\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m y_utm \u001b[38;5;241m=\u001b[39m summary_df\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m z_utm \u001b[38;5;241m=\u001b[39m summary_df\u001b[38;5;241m.\u001b[39mat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract 'x', 'y', 'z' from 'mean' column\n",
    "x_utm = summary_df.at['x', 'mean']\n",
    "y_utm = summary_df.at['y', 'mean']\n",
    "z_utm = summary_df.at['z', 'mean']\n",
    "\n",
    "# Transformer from (EPSG:32611 - UTM11N, WGS84) to (EPSG:4326 - lat, long, WGS84)\n",
    "latlon_proj = pyproj.Transformer.from_crs(32611, 4326, always_xy=True)\n",
    "\n",
    "# Transformer from (EPSG:4326 - lat, long, WGS84) to (EPSG:32611 - UTM11N, WGS84)\n",
    "utm_proj = pyproj.Transformer.from_crs(4326, 32611, always_xy=True)\n",
    "\n",
    "# Relative reference origin coordinates\n",
    "ref_latitude = 35.2\n",
    "ref_longitude = -118.2\n",
    "ref_depth = 0\n",
    "\n",
    "# Transform reference coordinates to UTM\n",
    "ref_longitude_utm, ref_latitude_utm = utm_proj.transform(ref_longitude, ref_latitude)\n",
    "\n",
    "# Correct relative reference origin effect\n",
    "x_utm += ref_longitude_utm\n",
    "y_utm += ref_latitude_utm\n",
    "\n",
    "# Convert depth from meters [m] to kilometers [km]\n",
    "z_depth = z_utm/1000\n",
    "\n",
    "# Transform coordinates from UTM to latitude, longitude\n",
    "x_longitude, y_latitude = latlon_proj.transform(x_utm,y_utm)\n",
    "\n",
    "# Print transformed coordinates\n",
    "print(f'Latitude: {x_longitude}째')\n",
    "print(f'Longitude: {y_latitude}째')\n",
    "print(f'Depth: {z_depth} [km]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e139b-ca99-4785-a648-0346dd54e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metolatlon(x_utm, y_utm, z_utm):\n",
    "    # Transformer from (EPSG:32611 - UTM11N, WGS84) to (EPSG:4326 - lat, long, WGS84)\n",
    "    latlon_proj = pyproj.Transformer.from_crs(32611, 4326, always_xy=True)\n",
    "\n",
    "    # Transformer from (EPSG:4326 - lat, long, WGS84) to (EPSG:32611 - UTM11N, WGS84)\n",
    "    utm_proj = pyproj.Transformer.from_crs(4326, 32611, always_xy=True)\n",
    "\n",
    "    # Relative reference origin coordinates\n",
    "    ref_latitude = 35.2\n",
    "    ref_longitude = -118.2\n",
    "    ref_depth = 0\n",
    "\n",
    "    # Transform reference coordinates to UTM\n",
    "    ref_longitude_utm, ref_latitude_utm = utm_proj.transform(ref_longitude, ref_latitude)\n",
    "\n",
    "    # Correct relative reference origin effect\n",
    "    x_utm += ref_longitude_utm\n",
    "    y_utm += ref_latitude_utm\n",
    "\n",
    "    # Convert depth from meters [m] to kilometers [km]\n",
    "    z_depth = z_utm / 1000\n",
    "\n",
    "    # Transform coordinates from UTM to latitude, longitude\n",
    "    x_longitude, y_latitude = latlon_proj.transform(x_utm, y_utm)\n",
    "\n",
    "    return x_longitude, y_latitude, z_depth\n",
    "\n",
    "# Example...\n",
    "x_utm_example = summary_df.at['x', 'mean']\n",
    "y_utm_example = summary_df.at['y', 'mean']\n",
    "z_utm_example = summary_df.at['z', 'mean']\n",
    "\n",
    "result = metolatlon(x_utm_example, y_utm_example, z_utm_example)\n",
    "\n",
    "print(f'Latitude: {result[0]}째')\n",
    "print(f'Longitude: {result[1]}째')\n",
    "print(f'Depth: {result[2]} [km]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab6a7dd-6348-47f3-94b3-ddf5531ce7ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc3EQT",
   "language": "python",
   "name": "pymc3eqt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
